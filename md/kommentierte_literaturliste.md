# Literaturliste

_Max Grüntgens_, März 2021

- [Literaturliste](#literaturliste)
  - [Vorbemerkung](#vorbemerkung)
    - [Ich habe nur Zeit für ein Buch!](#ich-habe-nur-zeit-für-ein-buch)
    - [Ich habe nur Zeit für drei Bücher!](#ich-habe-nur-zeit-für-drei-bücher)
    - [Ich habe Zeit für ein paar Bücher, will aber auch sofort praktisch arbeiten!](#ich-habe-zeit-für-ein-paar-bücher-will-aber-auch-sofort-praktisch-arbeiten)
  - [Retrodigitalisierung, "Verdatung" und Aufbau einer Sammlung](#retrodigitalisierung-verdatung-und-aufbau-einer-sammlung)
  - [Modellbildung](#modellbildung)
  - [Datenmodellierung](#datenmodellierung)
    - [Einstiegslektüre zum Textbegriff](#einstiegslektüre-zum-textbegriff)
    - [Einstiegslektüre zum "Digitalen Bild"](#einstiegslektüre-zum-digitalen-bild)
    - [Einstiegslektüre "Digitale Edition"](#einstiegslektüre-digitale-edition)
  - [Relationale Datenbank](#relationale-datenbank)
  - [Graphen und Netzwerke](#graphen-und-netzwerke)
  - [Linked Open Data (LOD) und Sematic Web](#linked-open-data-lod-und-sematic-web)
  - [Forschungs-Konzeption und Projekt-Arbeit](#forschungs-konzeption-und-projekt-arbeit)
  - [Data Science-Workflow, Analyse und Visualisierung](#data-science-workflow-analyse-und-visualisierung)
  - [Grundlagen Linguistik und Computerlinguistik](#grundlagen-linguistik-und-computerlinguistik)
  - [Empfohlene Meta-Lektüre](#empfohlene-meta-lektüre)
    - [Komplexitäts-Theorien](#komplexitäts-theorien)
  - [Programmierung](#programmierung)
    - [Python](#python)
    - [JavaScript](#javascript)

## Vorbemerkung

Die "Allgemeine Literaturliste" sammelt thematisch einführende Literatur zu einigen Themen im Kontext der <em>Digital Humanities</em>. Die Liste dient der eigenständigen Vertiefung und kann nur ein erster Einstieg in die Thematik sein.

### Ich habe nur Zeit für ein Buch!

Wenn Sie nur Zeit zur Lektüre eines einzigen Werks haben, dann starten Sie mit **Bod**, Rens (2015): _A New History of the Humanities._ Oxford. Bod gibt einen Überblick über das Für und Wider von quantitativen (formal-musterorientierten) und qualitativen (interpretativen) Ansätzen. Zahlreiche Beispiele stellen die durchaus schöpferische Spannung zwischen diesen zwei Extrempunkten dar. Die Lektüre von Bods Werk kann helfen, die vorherrschenden Arbeit- und die  Denkweisen in den Digital Humanities besser einordnen und nachvollziehen zu können.  

### Ich habe nur Zeit für drei Bücher!

Starten Sie mit **Bod**, Rens (2015): _A New History of the Humanities._ Oxford. Schließen Sie daran entweder **Jannidis**, F. et al.: _Digital Humanities. Eine Einführung_ an (wenn Sie einen knapperen aber breiten Überblick anstreben), oder **Flanders**, Julia / **Jannidis**, Fotis (2019): _The Shape of Data in Digital Humanities. Modeling Texts and Text-based Resources_ an (wenn Sie eine tiefergehende, aber auch anspruchvollere Auseinandersetzung bevorzugen). Im Anschluss sollten Sie Ihre Lektüre durch **Pomerantz**, Jeffrey (2015): _Metadata_. mit einem Metadaten-Fokus abschließen.

### Ich habe Zeit für ein paar Bücher, will aber auch sofort praktisch arbeiten! 

Konsultieren Sie alle oder einige der oben genannten Bücher, um sich ein grundlegendes Fundament zur Datenarbeit in den Digital Humanities zu machen. Wenn Sie dann direkt in die Praxis gehen wollen, sollten Sie sich für eine Art der Datenmodellierung entscheiden:

* relationale Datenbank: **DeBarros**, Anthony (2018): _Practical SQL. A Beginner’s Guide to Storytelling with Data_.
* Graphdatenbank: **Robinson**, Ian, **Webber**, Jim, **Eifrem**, Emil: _Graph Databases_. [https://neo4j.com/graph-databases-book/](https://neo4j.com/graph-databases-book/)
* TEI/XML: **Vogeler**, Georg / **Sahle**, Patrick (2017): _XML_, in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 128–146. und [TEI by Example](https://teibyexample.org/).

## Retrodigitalisierung, "Verdatung" und Aufbau einer Sammlung

* **DFG**: [_Merkblatt Erschließung und Digitalisierung_.](https://www.dfg.de/formulare/12_15/12_15_de.pdf) DFG-Vordruck 12.15–09/19.
* **DFG**: [_Praxisregeln „Digitalisierung_.](https://www.dfg.de/formulare/12_151/12_151_de.pdf) DFG-Vordruck 12.151–12/16.
* **Grüntgens**, **Kasper**, **Neuber**, **Prell**, **Schaßan**, **Stäcker**: Altbausanierung mit Niveau – die Digitalisierung gedruckter Editionen, Dhd 2020 Paderborn, S. 57–59, 10.5281/zenodo.3666690.

> Gibt einen Überblick über die Diskussion zur Retrodigitalisierung. Dort auch weiterführende Literatur.

* **Piotrowski**, Michael (2012): _Natural Language Processing for Historical Texts_. Toronto, S. 25–83.
* **Grüntgens**, Max / **Kollatz**, Thomas: _Korpusbasiertes Arbeiten und epigraphische Datenbanken: Möglichkeiten und Herausforderungen am Beispiel von EPIDAT und DIO._ In: Osnabrücker Beiträge zur Sprachtheorie (OBST), Nr. 92, 2018, S.157–173.

> Grüntgens und Kollatz gehen am Beispiel zweier Inschriften-Datenbanken des Mittelalters und der Frühen Neuzeit auf die mit historischen Sammlungen einhergehenden Herausforderungen ein: Unvollständigkeit, historische Sprachstufen, Varianz, Zweifelhaftigkeit u.a.m.     

* **O'Keeffe**, Anne / **McCarthy**, Michael (2012): _The Routledge Handbook of Corpus Linguistics._ 

> Umfassende Besprechung unterschiedlicher Aspekte korpuslinguistischer und sammlungsbasierter Forschung (historische Entwicklung, Theoriefundament, Aufbau einer Sammlung, sammlungsbasierte Forschung in verschiedenen Fachdisziplinen).   

* Projekt [OCR4all](https://www.uni-wuerzburg.de/en/zpd/ocr4all/)
* Projekt [Tesseract](https://tesseract-ocr.github.io/)
* **Stäcker**, Thomas: Die Sammlung ist tot, es lebe die Sammlung! Die digitale Sammlung als Paradigma moderner Bibliotheksarbeit. Bibliothek Forschung und Praxis | Band 43: Heft 2, 10.1515/bfp-2019-2066. 

## Modellbildung 

+ **Grim**, P. (2019): _Modelling Information_. In Floridi: The Routledge Handbook of Philosophy of Information, S. 137–152.

+ **James**, William (1907): Pragmatism: A New Name for some Old Ways of Thinking. New York, S. 51–58.

> James betont den Nützlichkeits- und Werkzeugcharakter von Theorien und Methoden. Eine wissenschaftliche Theorie sollte als Werkzeug verstanden werden, das einen Zweck zu erfüllen hat, um Handlungen zu erleichtern oder das Verständnis zu verbessern. Für James ist dieser Nützlichkeitsaspekt für alle unsere Konzepte zentral: Sie sind alle als Werkzeuge anzusehen, die danach beurteilt werden sollten, wie gut sie ihren beabsichtigten Zweck erfüllen.

+ **Leonelli**, S. (2019): _The Philosophy of Data_. In Floridi: The Routledge Handbook of Philosophy of Information, S. 119–202.

+ **Stachowiak**, Herbert (1973): _Allgemeine Modelltheorie_. Wien u.a., insb. S. 128–133.

> Stachowiak habt sich beide damit beschäftigt, wie Theorien und Modelle entstehen, benutzt und verändert werden. Er zeigt wie un warum ein Problem(feld) als Modell strukturiert wird und methodisch analysiert und daraufhin theoretisch wie methodisch be- bzw. verarbeitbar gemacht werden kann. 

+ **Luhmann**, Niklas (2018): _Die Wissenschaft der Gesellschaft_, S. 383–432.
  
> Luhmann exemplifiziert die Begriffe Modell, Methodik, Theorie, Daten u.a.m. im Kontext der Systemtheorie. 

Siehe auch den Punkt _Empfohlene Meta-Lektüre_ unten.

## Datenmodellierung

+ **Flanders**, Julia / **Jannidis**, Fotis (2019): _Data modeling in a digital humanities context_, in: Flanders, J./Jannidis, F.: The Shape of Data in Digital Humanities. Modeling Texts and Text-based Resources. London, S. 3–25.
+ **Flanders**, Julia / **Jannidis**, Fotis (2019): _A gentle introduction to data modeling_, in: Flanders, J./Jannidis, F.: The Shape of Data in Digital Humanities. Modeling Texts and Text-based Resources. London, S. 26–96.
+ **Jannidis**, Fotis (2017): _Grundlagen der Datenmodellierung_, in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 99–108.

> Flanders und Jannidis stellen die Grundlagen geisteswissenschaftlicher Datenmodellierung vor. Eine zentrale Aufgabe ist dabei die oft in nicht- oder nur semi-formalisierter Form vorliegenden Repräsentationen der Forschungsgegenstände in eine formalisiertere bzw. durch-formalisierte Form der Repräsentation zu überführen. 

+ **Pomerantz**, Jeffrey (2015): _Metadata_. Cambridge, 1–37.

> Pomerantz gibt einen Überblick über die Geschichte von "Metadaten", wie und warum sie eingesetzt wurden und werden und bespricht bekannte Modellierungs und Erschließungsansätze (Dublin Core, Dewey Decimal Classification) und Technologien (XML, Linked Open Data).  

### Einstiegslektüre zum Textbegriff

+ **Martens**, Gunter (2005): _Was ist ein Text? Ansätze zur Bestimmung eines Leitbegriffs der Textphilologie_, in: Stephan Kammer, Roger Lüdeke: Texte zur Theorie des Textes, Stuttgart, mit Hinführung, S. 91–113.

> Martens problematisiert den alltagssprachlichen Begriff "Text" und zeigt insbesondere, dass keine funktionalistische eins-zu-eins-Relation zwischen dem Lesen eines Textes (Input) und dem Verstehen eines Texte (Output) besteht, sondern dass das Lesen eines Textes zwar gewissen Beschränkungen oder *constraints* unterliegt und regelgeleitet abläuft, jedoch auch stets ein individueller, schöpferischer Akt ist. Vgl. zu dieser Konzeption auch Luhmann unten.     

+ **McKenzie**, D.F. (1999): _Bibliography and the Sociology of Texts._ Cambridge.

> McKenzie betont die Notwendigkeit breiter und tiefer Metadatisierung historischer Bestände. Er zeigt, dass ein "Text" nicht nur aus den lesbaren Buchstaben besteht, sondern auch der Objektcharakter und die Materialität des Textträgers hohe Signifikanz für viele kulturgeschichtliche Ansätze hat. Dieses Buch war richtungsweisend für die kulturwissenschaftliche Forschung. 

### Einstiegslektüre zum "Digitalen Bild"

* **Solem**, Jan Erik (2012): _Programming Computer Vision with Python_. Sebastopol.

> Solem gibt einen projektbasierten Überblick darüber, wie Bilder, als Rastergraphiken im Sinne von Matrizen, im Computer repräsentiert werden. Im Anschluss zeigt er an zahlreichen Beispielen wie Bilder als Daten analysiert und transformiert werden können, um einen Erkenntnisgewinn zu erzielen, e.g. Objekterkennung, 3D-Rekonstruktion, Augmented Reality, etc.

* **Kaehler**, Adrian, **Bradski**, Gary: _Learning OpenCV 3. Computer Vision in C++ with the OpenCV Library_. 3. Aufl. 2017. Sebastopol.

> Kaehler und Bradski erklären die Verarbeitung digitaler Rastergraphiken am Beispiel der Bibliothek OpenCV3 in C++. Nichtsdestoweniger ist das Buch auch als allgemeine Lektüre sinnvoll, da es problemzentriert vorgeht und die theoretischen wie praktischen Grundlagen für die Anwendung klar erläutert werden. Zum Einstieg sind  S. 3--6 und die kurzen Einführungstexte zu Beginn jedes Kapitels zur Lektüre zu empfehlen. 

### Einstiegslektüre "Digitale Edition"

Ein zentraler Arbeitsbereich in den digitalen Geisteswissenschaften ist die Erstellung und Bereitstellung digitaler Editionen und Textsammlungen. Hierbei synthetisiert "Digitale Editorik" Wissen und Traditionen der "analogen" Editionswissenschaft mit dem digitalen Arbeitsraum und einem informatischen Blick auf Texte, deren Konstitution und Bereitstellung.

+ **Sahle**, Patrick (2017): _Digitale Edition_, in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 234–248.
+ **Vogeler**, Georg / **Sahle**, Patrick (2017): _XML_, in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 128–146.

> Sahle und Vogeler stellen die Grundlagen der in den Digitalen Geisteswissenschaften sehr verbreiteten _Extensible Markup Language_ (XML) vor. Außerdem vermittelt Sahle die Grundlagen der Digitalen Editorik bzw. Edition, in deren Kontext häufig auf XML zurückgegriffen wird. 

+ **Plachta**, Bodo (1997): _Editionswissenschaft: Eine Einführung in Methode und Praxis der Edition neuerer Texte_. Stuttgart, 3. Aufl.

## Relationale Datenbank

+ **Klinke**, Harald (2017): _Datenbanken_, in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 109–127.
+ **DeBarros**, Anthony (2018): _Practical SQL. A Beginner’s Guide to Storytelling with Data_.

> Praxisorientierte Einführung in die Arbeit mit einer relationalen Datenbank. Auch für Anfänger geeignet.  

## Graphen und Netzwerke

+ **Fotis**, Jannidis: _Netzwerke_ , in: Jannidis, F. et al.: Digital Humanities. Eine Einführung. Stuttgart, S. 147–161.
+ **Needham**, Mark, **Hodler**, Amy: _Graph Algorithms: Practical Examples in Apache Spark and Neo4j_. [https://neo4j.com/graph-algorithms-book/](https://neo4j.com/graph-algorithms-book/)
+ **Hunger**, Michael: _Neo4j 2.0 – Eine Graphdatenbank für alle._ [https://neo4j.com/neo4j-graphdatenbank-book/](https://neo4j.com/neo4j-graphdatenbank-book/) 

> Eine projektorientierte, problemzentrierte Einführung, die insbesondere für Anfänger geeignet ist. Die "Denke hinter Graphen" wird pointiert erläutert und am Beispiel einer Film-Datenbank expliziert.

+ **Kuczera**, Andreas: _Graphentechnologien in den digitalen Geisteswissenschaften. Modellierung – Import – Analyse_. [https://kuczera.github.io/Graphentechnologien/](https://kuczera.github.io/Graphentechnologien/)

> Kuczeras webbasiertes Buch bietet einen genuien geisteswissenschaftlichen Blick auf Graphen-Technologien. Der Dreischritt Modelierung–Import–Analyse wird mit vielen nachnutzbaren Code-Beispielen praxisnah erläutert.

+ **Robinson**, Ian, **Webber**, Jim, **Eifrem**, Emil: _Graph Databases_. [https://neo4j.com/graph-databases-book/](https://neo4j.com/graph-databases-book/)

> Ein Buch, das sehr gut im Anschluss an Hungers oder Kuczeras Publikationen gelesen werden kann, um Aspekte zu vertiefen und zu schärfen. 

## Linked Open Data (LOD) und Sematic Web

* **Allemang**, Dean / **Hendler**, James (2011): _Semantic Web for the Working Ontologist: Effective Modeling in RDFS and OWL._ 

> Grundlagenwerk für alle die im Zentrum von Semantic Web und LOD arbeiten wollen, indem sie eigene Ontologien erstellen oder bestehende erweitern.

* **Eide**, Ø. / **Ore**, C.-E. (2019). _8. Ontologies and Data Modeling._ In Flanders, J., Jannidis, F. (eds), The Shape of Data in Digital Humanities Modeling Texts and Text-based Resources . London, S. 178–196.

> Eine anfängerfreundliche, grundlegende Einführung in das Thema LOD/Semantic Web für Geisteswissenschaftler:innen. Ähnlich basale, aber kürzere Einführungen bieten auch Meinderstma und Rehbein (s.u.).

* **Grüntgens**, M. / **Schrade**, T. (2016). _Data repositories in the Humanities and the Semantic Web: modelling, linking, visualising._ In Alessandro, A. et al. (eds), Proceedings of the 1st Workshop on Humanities in the Semantic Web co-located with 13th ESWC Conference 2016 (ESWC 2016), Anissaras, S. 53–64 URL: [http://ceur-ws.org/Vol-1608/paper-07.pdf](http://ceur-ws.org/Vol-1608/paper-07.pdf)

> Schrade und Grüntgens zeigen wie XML-basierte Sammlungen auf einfache Art und Weise in Datenformate des Semantic Web übertragen werden können und welche neuen Möglichkeiten eine deratige Übertragung bietet.

* **Meinderstma**, Joep: [_A brief introduction to linked data_.](https://ontola.io/what-is-linked-data/) 
* **Rehbein**, Malte: _Ontologien_, in: Jannidis et al.: Digital Humanities. Eine Einführung. S. 162–176.
* **Schrade**, T. (2016): [_XTriples. A generic webservice for RDF lifting from XML resources._](https://zenodo.org/record/2604986#.XMgdp5MzaAw) 

> Webservice zur Transformation von XML in, bzw. zur Extrakion von LOD/Semantic Web Datenformaten aus XML.

## Forschungs-Konzeption und Projekt-Arbeit 

* **Evans**, Eric (2003): _Domain-Driven Design.Tackling Complexity in the Heart of Software_. Boston.

> Eine der "Bibeln" der Software-Entwicklung. Evans betont neben der Notwendigkeit klarer und konsistenter Problem-Analysen vor allem das Miteinander mit dem Nutzer der Software, dem sog. Domänenexperten. Damit das Miteinander kommunikativ überhaupt erfolgreich sein kann, muss zwischen allen Projekt-Mitarbeitern eine sog. "ubiquitäre Sprache" entwickelt werden, also eine Sprache, die allgemein im gesamten Projektkontext von allen Beteiligten verwendet wird, eine "Lingua Franca".   

* **Grüntgens**, Max / **Kasper**, Dominik (2017): [_Nachhaltige Konzeptionsmethoden für Digital Humanities Projekte am Beispiel der Goethe-PROPYLÄEN_](http://www.dhd2017.ch/wp-content/uploads/2017/01/Abstractband_def_24.1.17-1.pdf). Bern, S. 165–167.
* **Schrade**, Torsten (2017): [_Nachhaltige Softwareentwicklung in den Digital Humanities. Konzepte und Methoden_](http://www.dhd2017.ch/wp-content/uploads/2017/01/Abstractband_def_24.1.17-1.pdf). Bern, S. 168–171.

> Grüntgens, Kasper und Schrade wenden das von Evans entwickelte Schema auf die geisteswissenschaftliche Forschung an und erklären einzelne Schritte an einem dezidierten "real life" Projekt. Außerdem binden sie das Konzept des *Behaviour Driven Developments* (BDD) mit in ihre Überlegungen ein.  

* [AG Research Software Engineering in den Digital Humanities (im DHd-Verband)](https://dh-rse.github.io/) 

> Die AG Research Software Engineering in den Digital Humanities (DH-RSE) möchte die Position von Softwareentwickler_innen in den Geistes- und Kulturwissenschaften stärken und ihre Forschungsbeiträge innerhalb der Digital Humanities besser sichtbar machen. (Selbsbeschreibung auf der Website)

## Data Science-Workflow, Analyse und Visualisierung

* **Aydelotte**, William O. (1971): _Quantification in history._ Reading, Mass.

> Bespricht Vor- und Nachteile quantifizierender und damit häufig sammlungsbasierter Ansätze in der Geschichtswissenschaft. Hintergrund zur Publikation war der zunehmende Druck (und die verbreitete Ablehnung quantitativer Ansätze) durch die "traditionell-hermeneutische" Geschichtswissenschaft.

* **Alt**, Mick: _Exploring Hyperspace. *A non-mathematical explanation of multivariate analysis.*_ 1990.

> Nicht-mathematische Einführung in multivariate Analyseansätze.  

* **Bengfort**, Benjamin / **Bilbro**, Rebecca / **Ojeda**, Tony (2018): _Applied Text Analysis with Python._ 

> Problemorientierte Hands-On-Einführung in die Sammlung, Aufbereitung, Verarbeitung und Analyse (großer) Text-Sammlungen mit der Programmiersprache Python und ausgewählten Bibliotheken (NLTK, Scikit, spaCy, u.a.m.). Wenn man Python verwenden will, ist diese Buch ein Must-Have, gegebenenfalls komplementiert durch die Python-Machine/Deep-Learning-Bücher von O'Reilly. 

* **Cody**, Field: _The Data Science Handbook._ 2017.

> Grundlegende Einführung in _Data Science_ und Überblick über den Gesamtworkflow sowie über die einzelnen Aufbereitungs-, Verarbeitungs- und  Analyse-Tasks.

* **Jockers**, Matthew L. (2013): _Macroanalysis. Digital Methods & Literary History._ Urbana.

> Gibt einen Überblick über die Möglichkeiten zur Analyse von bibliographischen Metadaten-Sammlungen, aber auch von Texten und Text-Sammlungen im geisteswissenschaftlichen, meist literaturwissenschaftlichen Kontext. Da der Benutzung bestimmter Methoden immer  eine dezidierte geisteswissneschaftliche Fragestellung zugrundeliegt, ist es ein guter Einführungstext, wenn man Beispiele für digitale Geisteswissenschaftliche Forschung sucht. 

* **Fisher**, Daniel, **Meyer**, Miriah: _Making Data Visual._ 2017.

> Kleines Büchlein zur Theorie hinter der Datenvisualisierung und der Notwendigkeit einer adäquaten Operationalisierung.

* **Kenny**, Anthony (1982): _The Computation of Style. An Introduction to Statistics for Students of Literature and Humanities._ New York. 

> Ein Text der sich dezidiert an Geisteswissenschaftler richtet und diesen einen "statistischen" Blick auf ihre Fragestellungen und ihre Forschungsgegenstände vermitteln will. Daher ist hier die Vermittlung statistischen Grundwissens immer stark mit der adäquaten Formalisierung einer geisteswissenschaftlichen Fragestellung und anschließender Operationalisierung auf Basis dieser Formalisierung verzahnt.   

* **Krippendorff**, Klaus (2019): _Content Analysis. An Introduction to Its Methodology._ Los Angeles.

> Die ersten Kapitel geben eine Einführung in die Geschichte der Inhaltsanalyse. Verschiedene Inhalts- bzw. Informationstheorien werden besprochen und der Kontext in den die Forschungsobjekte eingebettet sind unter den Schlagworten _Text_, _Forschungsfrage_, _Kontext_, _Analytische Konstrukte_ und _Validierung_, dargestellt.  

* **Lemercier**, Claire / **Zalc**, Claire (2019): _Quantitative Methods in the Humanities. An Introduction._  

> Gibt einen kurzen historischen Abriss über die Geschichte quantitativer Ansätze in den Geisteswissenschaften (mit Fokus auf der Geschichtswissenschaft). Anschließend werden Datenerhebung sowie verschiedene analystische Methoden (deskriptive und inferentielle Statistik, Visualisierung, Netzwerke) an hand von Beispielen durchgesprochen.  

* **MacEachren**, Alan: _How Maps Work._ 1995.

> Grundlegende Einführung in die Theorie und Praxis von Karten und Kartierung.

* **Moretti**, Franco: _‘Operationalizing’: or, the Function of Measurement in Modern Literary Theory._ In: Literary Lab Pamphlet 6 (2013), pp. 1–13. ([online](https://litlab.stanford.edu/LiteraryLabPamphlet6.pdf))

> Formalisierung und Operationalisierung an dezidiert geisteswissenschaftlichen Beispielen (Dramen).

* **Segaran**, Toby (2007): _Programming Collective Intelligence_.

> Segaran beschreibt und implementiert verschiedene Algorithmen aus dem _machine learning_-Kontext, um Datensammlungen zu filtern, zu clustern oder allgemein auf maschinelle Weise _insights_ zu generieren.  

* **Pennebaker**, James W. (2013): _The Secret Life of Pronouns. What our Words Say about Us._ 

> Pennebaker zeigt an verschiedenen Beispielen auf, dass Texten sehr unterschiedliche Eigenschaften zukommen können, je nachdem aus welchem Blickwinkel, mit welcher Fragestellung und mit welcher Methode sie prozessiert werden: so werden beim Lesen und Schreiben, viele Wörter nur unterbewusst verarbeitet oder gebraucht und die Präferenz für oder gegen spezifische Worte oder Wortkombinationen bspw. mit Faktoren wie Gender oder Alter korrellieren können.   

* **Tuckey**, John W.: _Exploratory Data Analysis._ 1977.

> Der Klassiker der explorativen Datenvisualisierung. 

## Grundlagen Linguistik und Computerlinguistik

* **Biber**, Douglas, **Conrad**, Susan, **Reppen**, Randi: _Corpus Linguistics. Investigating Language Structure and Use_. 1998.

> Klassischer, einführender Text in die Korpuslinguistik. Die einzelnen Kapitel fokussieren jeweils auf spezifische Themen wie Lexikographie, Grammatik, Diskurs, Register-Variation/Soziolekte. Jedes Kapitel folgt dem Schema: 1) Identifikation und Klärung der Forschungsfrage, 2) Aufbereitung und Analyse, 3) Interpretation der Analyse-Ergebnisse.

* **Lobin**, Henning (2010): _Computerlinguistik und Texttechnologie._ 

> Ein knapper, aber sehr konziser Durchmarsch durch das Thema. Spricht neben Verarbeitung und Analyse von Texten und bestehenden Sammlungen auch die Aufbereitung und Bereitstellung von (linguistischen) Text-Sammlungen an.  

* **Lyons**, John (1995): _Einführung in die moderne Linguistik._ München.
* **Pelz**, Heidrun (2013): _Linguistik. Eine Einführung._ Hamburg.

## Empfohlene Meta-Lektüre

+ **Bod**, Rens (2015): _A New History of the Humanities._ Oxford. 

> Gibt einen historischen Überblick über die Entwicklung geisteswissenschaftlicher Forschung und Forschungsansätze von der Antike bis heute. Dabei fokussiert Bod auf Forschungsansätze, die darauf abzielen Muster und Strukturen zu erkennen, diese herauszuarbeiten und für Analysen zu nutzen. Als überblicksartige Einführung in die Geisteswissenschaften und ihre Forschungsegenstände geeignet.      

* **Gleick**, James: _The Information_. 2012. London.

> Sehr gut lesbare "populäre" Einführung in den Informations-Begriff und seine Geschichte. Gleick fasst die theoretischen die praktischen Aspekte des Informations-Begriffs und erläutert distinkte theoretische Punkte in klarer Sprache. 

* **Luhmann**, Niklas (2017): _Einführung in die Systemtheorie_. Heidelberg.

> Luhmann zeigt wie man in sehr strukturierter Art und Weise über Wahrnehmung, Theorien und Theoriebildung nachdenken kann. Er setzt das Individuum als Beobachtungssystem in den Mittelpunkt seiner Überlegungen und macht deutlich, wie wichtig es ist, bei beobachtetem und wahrgenommenem immer danach zu fragen wer (oder was) eigentlich der Beobachter ist.

+ **Stachowiak**, Herbert (1973): Allgemeine Modelltheorie. Wien u.a..

> Luhmann und Stachowiak haben sich beide damit beschäftigt, wie Theorien und Modelle entstehen, benutzt und verändert werden. Beide zeigen wie ein Problem(feld) strukturiert und methodisch analysiert und daraufhin theoretisch wie methodisch be- bzw. verarbeitbar gemacht werden kann. Wer sich für Modell- und Theoriebildung interessiert, sollte diese zwei "Altmeister" lesen.    

### Komplexitäts-Theorien

Systeme können als einfache, komplizierte und komplexe Systeme verstanden werden. Bei einfachen Systemem handelt es sich um solche die durch lineare Gleichungen darstellbare sind, komplexe Systeme sind nicht-linear. Durch Theoriebildung können nicht-lineare Systeme jedoch in lineare transformiert werden, um deren Verarbeitung zu erleichtern. Systeme sind dann komplex, wenn sie divers/heterogen, verbunden/vernüpft, interdependent und adaptiv sind. Komplizierte Systeme können ebenfalls heterogen und verbunden sein, sind jedoch oft nicht interdependent und nie adaptiv. Ein weiterer zentraler Punkt komplexer Systeme ist ihre Fähigkeit emergente Phänomene zu erzeugen, e.g. das komplexe System Gehirn/Nervensystem erzeugt das emergente Phänomen "Bewusstsein/Geist". Weitere Beispiele für komplexe Systeme sind Sprache, Kultur, Hermeneutik, eine Schlacht zwischen zwei Heeren u.v.m.

* **Johnson**, Neil: _Simple Complexity. A Clear Guide to Complexity Theory_. London 2017.

> Johnson gibt eine sehr gut lesbare "populäre" Einführung in das Thema Komplexitätstheorie.

* **Mitchell**, Melanie: _Complexity. A Guided Tour_. Oxford 2011. 

> Mitchell gibt eine gut lesbare, minimal mathematische _tour de force_ durch das Thema Komplexitätstheorie. 

* **Resnick**, Mitchel: _Turtles, Termites, and Traffic Jams. Explorations in Massively Parallel Microworlds_. 7. Aufl. 2001.

> Resnick beschreibt komplexe Systeme und emergente Phänomene am Beispiel von Simulationen von Schleim-Pilzen und Ameisen u.a.m. 



## Programmierung

### Python

* **Matthes**, Eric (2019): _Python Crash Course, 2nd Edition: A Hands-On, Project-Based Introduction to Programming._

> Python-Kurs für Anfänger:innen, der die Sprache anhand von kleinen Projekten vermittelt.

* **Chen**, Daniel Y. (2018): _Pandas for Everyone. Python Data Analysis._ 

> Grundlegender Einstieg und Durchmarsch durch die Data Science-Bibliothek "Pandas".

### JavaScript

* **Haverbeke**, Marijn: _Eloquent JavaScript 3rd Edition._ 2019. (auch frei online verfügbar)

> Die derzeit beste Einführung in JavaScript.

* **Murray**, Scott: _Interactive Data Visualization for the Web._ 2nd Ed. 2017.

> Grundlegender Kurs zum Arbeiten mit dem D3-Framework für Datenvisualisierung.



